# NOC Runbooks - Guia de Resposta a Alertas

Este documento descreve os passos que o NOC deve seguir quando alertas de edge functions sÃ£o disparados.

---

## ğŸ”´ TIER 1 CRÃTICO: Alerta Imediato

**Severidade**: CRITICAL  
**DestinatÃ¡rio**: PagerDuty + Slack #critical  
**AÃ§Ã£o**: Page dev on-call imediatamente  
**SLA**: Response em < 5 minutos

### FunÃ§Ãµes CrÃ­ticas

1. `whatsapp-send` - Envio de mensagens
2. `track-user-usage` - Rastreamento de uso/billing
3. `empresas-list` - Base de dados (dependency)
4. `relatorios-dre` - RelatÃ³rio financeiro
5. `relatorios-cashflow` - Fluxo de caixa
6. `reconcile-bank` - ConciliaÃ§Ã£o bancÃ¡ria
7. `whatsapp-conversations` - HistÃ³rico de chats
8. `financial-alerts-update` - ResoluÃ§Ã£o de alertas
9. `onboarding-tokens` - Controle de acesso

---

## Runbook: TIER 1 - FunÃ§Ã£o Down (HTTP 50x ou timeout)

### Sintomas

```
Alert: whatsapp-send returned 503 (Service Unavailable)
Status: RED
Tier: 1 (CRITICAL)
```

### Resposta Imediata (1-2 min)

```
â˜ 1. Reconhecer alerta em PagerDuty (click "Acknowledge")
â˜ 2. Abrir Slack #incident e mencionar @on-call
â˜ 3. Rodar diagnÃ³stico rÃ¡pido:

   ./test-all-edge-functions.sh --tier 1 --output json | \
     jq '.results[] | select(.name=="whatsapp-send")'

â˜ 4. Verificar status da funÃ§Ã£o no Supabase:

   # Login ao painel Supabase
   # Edge Functions â†’ whatsapp-send â†’ Logs
   # Procurar erros nos Ãºltimos 5 minutos

â˜ 5. Se erro for "Deployment error":
   â†’ Ir para "Deploy Issue" abaixo

â˜ 6. Se erro for "Database connection":
   â†’ Ir para "Database Issue" abaixo

â˜ 7. Se erro for "External API timeout":
   â†’ Ir para "External Integration Issue" abaixo
```

### Deploy Issue

```
Se log mostrar "Failed to load dependency" ou similar:

â˜ 1. SSH para o servidor (ou acesse via Supabase CLI):
   supabase functions deploy whatsapp-send

â˜ 2. Verificar compilaÃ§Ã£o:
   supabase functions list | grep whatsapp-send

â˜ 3. Se ainda falhar, reverter versÃ£o anterior:
   git log --oneline supabase/functions/whatsapp-send
   # Anotar o commit da Ãºltima versÃ£o estÃ¡vel
   
   git checkout <commit-hash> -- \
     supabase/functions/whatsapp-send
   
   supabase functions deploy whatsapp-send

â˜ 4. Re-testar:
   ./test-all-edge-functions.sh --tier 1
   
â˜ 5. Se OK â†’ Fecha alerta em PagerDuty
   Se nÃ£o â†’ Escala para developer (vÃ¡ para Developer Escalation)
```

### Database Issue

```
Se log mostrar "Connection timeout" ou "Database refused connection":

â˜ 1. Verificar status do Supabase:
   https://status.supabase.io/

â˜ 2. Se status Ã© GREEN, problema Ã© local:
   
   # Verificar RLS policies
   SELECT COUNT(*) FROM pg_policies WHERE tablename = 'whatsapp_messages';
   
   # Se 0 rows â†’ RLS policies podem estar quebradas
   â†’ VÃ¡ para "RLS Policy Issue"

â˜ 3. Se status mostra incident:
   â†’ Postear em Slack com link
   â†’ Aguardar resoluÃ§Ã£o (status page)
   â†’ Teste novamente em 5 min

â˜ 4. Se problema persiste > 10 min:
   â†’ Escala para Supabase support (vÃ¡ para External Escalation)
```

### External Integration Issue

```
Se log mostrar "Failed to call wasender API" ou similar:

â˜ 1. Verificar status do serviÃ§o externo:
   https://status.wasender.com/ (ou seu provider)

â˜ 2. Testar conectividade:
   curl -I https://api.wasender.com/health

â˜ 3. Se falha com timeout:
   â†’ Aumentar timeout na funÃ§Ã£o (vÃ¡ para Developer Escalation)
   â†’ Documentar no Slack que serÃ¡ melhorado

â˜ 4. Se falha com 401/403:
   â†’ Verificar credenciais (API key expirada?)
   â†’ Rotacionar credenciais se necessÃ¡rio
   â†’ Redeploy com novas credenciais

â˜ 5. Se problema persiste:
   â†’ Postear no Slack que webhook/integraÃ§Ã£o estÃ¡ down
   â†’ Aguardar fix do provider
```

---

## Runbook: TIER 1 - LatÃªncia Alta (P95 > 10s)

### Sintomas

```
Alert: whatsapp-send P95 latency = 12.5s (threshold: 10s)
Status: YELLOW
Tier: 1 (CRITICAL - latÃªncia crÃ­tica)
```

### Resposta Investigativa (2-5 min)

```
â˜ 1. Rodar teste de latÃªncia isolado:
   time curl -X POST \
     https://your-supabase.url/functions/v1/whatsapp-send \
     -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"phone":"test","message":"test","company_cnpj":"test"}'

â˜ 2. Coletar dados de latÃªncia (Ãºltimas 20 chamadas):
   SELECT
     response_time_ms,
     PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95
   FROM health_checks
   WHERE function_name = 'whatsapp-send'
   AND timestamp > NOW() - INTERVAL '30 minutes'

â˜ 3. Identificar padrÃ£o:
   - Se latÃªncia Ã© consistente > 10s â†’ problema de resource
   - Se latÃªncia Ã© spike occasional â†’ problema de load/throttling
   - Se latÃªncia estÃ¡ crescendo â†’ problema de query/database

â˜ 4. AÃ§Ã£o baseada em padrÃ£o:

   SE consistente (sempre > 10s):
     â†’ Ir para "Resource Issue"

   SE spike (occasional peaks):
     â†’ Ir para "Load/Throttling Issue"

   SE crescendo (trend up):
     â†’ Ir para "Database Query Issue"
```

### Resource Issue

```
Se funÃ§Ã£o sempre Ã© lenta:

â˜ 1. Verificar recursos na funÃ§Ã£o Supabase:
   Edge Functions â†’ whatsapp-send â†’ Monitoring
   Procurar CPU high, memory high

â˜ 2. Se CPU/Memory OK:
   â†’ Problema Ã© externo (wasender, database, network)
   â†’ Ir para "External Integration Issue"

â˜ 3. Se CPU/Memory high:
   â†’ Revisar funÃ§Ã£o por loops desnecessÃ¡rios
   â†’ Verificar se hÃ¡ mÃºltiplas iteraÃ§Ãµes (for loop sem break)
   
â˜ 4. Se cÃ³digo estÃ¡ OK:
   â†’ Aumentar memory allocation (Supabase settings)
   â†’ Redeploy
   â†’ Retest

â˜ 5. Se problema persiste:
   â†’ Escala para dev (vÃ¡ para Developer Escalation)
```

### Load/Throttling Issue

```
Se latÃªncia Ã© ocasional (spike):

â˜ 1. Verificar volume de requisiÃ§Ãµes:
   SELECT
     DATE_TRUNC('minute', timestamp) as minute,
     COUNT(*) as count
   FROM health_checks
   WHERE function_name = 'whatsapp-send'
   AND timestamp > NOW() - INTERVAL '1 hour'
   GROUP BY minute
   ORDER BY minute DESC

â˜ 2. Correlacionar com SLA:
   - Se spike coincide com latÃªncia alta â†’ throttling
   - Supabase pode estar rate-limiting

â˜ 3. AÃ§Ã£o:
   â†’ Documentar spike em Slack
   â†’ Implementar backoff/retry logic
   â†’ Considerar pre-warming de funÃ§Ã£o (chamada periÃ³dica)

â˜ 4. Longo prazo:
   â†’ Escala com dev para otimizar funÃ§Ã£o
   â†’ Considerar caching se possÃ­vel
```

### Database Query Issue

```
Se latÃªncia estÃ¡ crescendo (trend):

â˜ 1. Executar EXPLAIN ANALYZE na query lenta:
   SELECT * FROM whatsapp_messages
   WHERE company_cnpj = '...'
   ORDER BY created_at DESC
   LIMIT 100

   (Adicionar EXPLAIN ANALYZE na frente)

â˜ 2. Verificar Ã­ndices:
   SELECT * FROM pg_indexes
   WHERE tablename = 'whatsapp_messages'

â˜ 3. Se faltam Ã­ndices relevantes:
   CREATE INDEX idx_whatsapp_messages_company
   ON whatsapp_messages(company_cnpj, created_at DESC)

â˜ 4. Se Ã­ndices existem:
   â†’ Problema pode ser vacuum/bloat da tabela
   â†’ Executar VACUUM ANALYZE:
   
   VACUUM ANALYZE whatsapp_messages;

â˜ 5. Se problema persiste:
   â†’ Escala para DBA/dev
   â†’ Considerar table partitioning
```

---

## ğŸŸ¡ TIER 2 MÃ‰DIO: Alerta Monitorado

**Severidade**: MEDIUM  
**DestinatÃ¡rio**: Slack #alerts  
**AÃ§Ã£o**: Investigar, escalar se > 30 min down  
**SLA**: Investigar em < 15 min

### FunÃ§Ãµes MÃ©dias

1. `llm-chat` - IA conversacional
2. `mood-index-timeline` - Analytics
3. `rag-search` - Search
4. `n8n-status` - AutomaÃ§Ãµes
5. E mais 5 funÃ§Ãµes...

---

## Runbook: TIER 2 - FunÃ§Ã£o Down

```
Alert: llm-chat returned 500
Status: RED
Tier: 2 (MEDIUM)

â˜ 1. Verificar se Ã© transiente:
   ./test-all-edge-functions.sh --tier 2 | grep llm-chat
   
   Se OK na retest â†’ foi spike, close alert

â˜ 2. Se persiste, seguir steps de TIER 1:
   - Deploy Issue
   - Database Issue
   - External Integration Issue

â˜ 3. Timeline:
   - 0-5 min: InvestigaÃ§Ã£o
   - 5-15 min: Tentativa de fix local
   - 15-30 min: Escalar para dev
   - > 30 min: PagerDuty page (CRITICAL escalation)
```

---

## ğŸŸ¢ TIER 3 ADMIN: On-Demand

**Severidade**: LOW  
**DestinatÃ¡rio**: Debug log only  
**AÃ§Ã£o**: Nenhuma (testes)  

```
FunÃ§Ãµes TIER 3 nÃ£o geram alertas automÃ¡ticos.
Se falha: investigar em CI/CD ou staging, sem impacto em produÃ§Ã£o.
```

---

## Developer Escalation

Use este procedimento quando NOC nÃ£o consegue resolver:

```
Slack message:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@on-call-dev

ğŸš¨ TIER 1 ALERT: whatsapp-send

Status: Down (HTTP 503)
Duration: 15 minutes
Last working: 2025-11-09 14:20 UTC

Diagnosticado:
- Deploy status: OK
- Database: Connected
- External API: Responding

AÃ§Ã£o necessÃ¡ria:
1. Revisar logs em Supabase Edge Functions
2. Check for recent deploys or config changes
3. Rollback se needed

Logs anexados: [link]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## External Escalation

Para integraÃ§Ã£o com suporte externo:

### Wasender (WhatsApp Provider)

```
Email: support@wasender.com
Subject: URGENT: API down - Order #XXXXX

Body:
"WhatsApp send function returning 500.
Our service was working fine until 14:20 UTC.
External API health check shows timeout.
Need immediate status update."

Cc: on-call-dev
```

### Supabase

```
Dashboard: https://supabase.io/support
Ticket Type: Urgent
Subject: Edge Function / Database connectivity

Include:
- Function name
- Error message
- Timestamps
- Health check results (JSON)
```

---

## Post-Incident

ApÃ³s resolver qualquer alerta TIER 1:

```
â˜ 1. Postar resumo em Slack #incident:
   - DuraÃ§Ã£o total
   - Root cause
   - AÃ§Ã£o tomada
   - Status: RESOLVED

â˜ 2. Criar ticket (se necessÃ¡rio):
   - Criar issue no GitHub para tracking
   - Assign para dev para follow-up
   - Label com "incident", "tier-1", etc.

â˜ 3. Documentar em runbook:
   - Se foi novo cenÃ¡rio â†’ adicionar Ã  este doc
   - Se foi pattern conhecido â†’ nota para melhoria

â˜ 4. Verificar trends:
   - Se mesma funÃ§Ã£o falhou antes:
     â†’ Agendou fix permanente?
     â†’ Por que ainda falha?
```

---

## Checklist DiÃ¡rio do NOC

```
Start of shift:
â˜ Revisar health_checks_summary (Ãºltimas 24h)
â˜ Verificar se hÃ¡ alertas pendentes
â˜ Rodar ./test-all-edge-functions.sh --tier 1
â˜ Documentar status inicial em Slack

During shift:
â˜ Monitorar dashboard (health check status)
â˜ Responder a alerts conforme SLA
â˜ Atualizar Slack periodicamente
â˜ Registrar issues no GitHub se necessÃ¡rio

End of shift:
â˜ Preparar handoff para turno seguinte
â˜ Listar issues abertas/pendentes
â˜ Sumarizar health status (% uptime)
â˜ Escaladas que ficaram abertas
```

---

## Contatos de EscalaÃ§Ã£o

| ServiÃ§o | Contato | HorÃ¡rio | SLA |
|---------|---------|---------|-----|
| Dev On-Call | @on-call-dev (Slack) | 24/7 | 5 min |
| Wasender | support@wasender.com | Business hours | 2 hours |
| Supabase | Dashboard support | Business hours | 4 hours |
| AWS | Account manager | 24/7 | 1 hour |

---

## MÃ©tricas para Dashboard

Crie um dashboard que mostra:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NOC METRICS - Semana 45 (2025-11-03 a 2025-11-09)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚ Uptime por Tier:                                   â”‚
â”‚ â”œâ”€ TIER 1: 99.85% (SLA: 99.5%) âœ…                 â”‚
â”‚ â”œâ”€ TIER 2: 98.92% (SLA: 98%) âœ…                   â”‚
â”‚ â””â”€ TIER 3: 99.10% (SLA: N/A) âœ…                   â”‚
â”‚                                                     â”‚
â”‚ Alertas:                                           â”‚
â”‚ â”œâ”€ Total: 12                                       â”‚
â”‚ â”œâ”€ Resolvidos: 11                                  â”‚
â”‚ â”œâ”€ MTTR (Mean Time To Resolve): 18 min            â”‚
â”‚ â”œâ”€ EscalaÃ§Ãµes: 2                                   â”‚
â”‚ â””â”€ False positives: 0                              â”‚
â”‚                                                     â”‚
â”‚ Top Issues:                                        â”‚
â”‚ 1. llm-chat latÃªncia alta (3 ocorrÃªncias)         â”‚
â”‚ 2. sync-bank-metadata timeout (2 ocorrÃªncias)     â”‚
â”‚ 3. reconcile-bank occasional 503 (1 ocorrÃªncia)   â”‚
â”‚                                                     â”‚
â”‚ AÃ§Ã£o recomendada:                                  â”‚
â”‚ - Investigar llm-chat latÃªncia (dev meeting)      â”‚
â”‚ - Aumentar timeout sync-bank-metadata             â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


